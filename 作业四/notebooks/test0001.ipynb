{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c680b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入常用库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "#设置显示中文\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "# 无视警告\n",
    "import warnings \n",
    "# warnings.filterwarnings('always') # 显示所有警告\n",
    "warnings.filterwarnings('ignore') # 忽略所有警告\n",
    "\n",
    "\n",
    "def data_rename(data):\n",
    "    '''\n",
    "    重新命名列名，帮助我这个不会英语的人更好理解数据\n",
    "    :param data: 待处理的数据\n",
    "    :return: 重新命名后的数据'''\n",
    "    # 检查列名\n",
    "    print(data.columns.tolist())\n",
    "    # 创建英文列名到中文列名的映射字典\n",
    "    column_mapping = {'City Name': '城市名称','Type': '类型','Package': '包装','Variety': '品种','Sub Variety': '子品种','Grade': '等级','Date': '日期','Low Price': '最低价格','High Price': '最高价格','Mostly Low': '主要最低价','Mostly High': '主要最高价', 'Origin': '产地','Origin District': '产地区域','Item Size': '物品尺寸','Color': '颜色','Environment': '环境','Unit of Sale': '销售单位','Quality': '质量','Condition': '状况','Appearance': '外观','Storage': '储存','Crop': '作物','Repack': '重新包装','Trans Mode': '运输模式','Unnamed: 24': '未命名: 24','Unnamed: 25': '未命名: 25'}\n",
    "    # 替换DataFrame的列名\n",
    "    data.rename(columns=column_mapping, inplace=True)\n",
    "    return data\n",
    "def date_chuli0(data):\n",
    "    '''\n",
    "    将日期列转换为日期格式\n",
    "    :param data:\n",
    "    :return:日期格式化后的数据\n",
    "    '''\n",
    "    # data_rename(data)\n",
    "    # 将日期列转换为日期格式\n",
    "    data['日期'] = pd.to_datetime(data['日期'], format='%m/%d/%y')\n",
    "    data[\"年份\"] = data['日期'].dt.year\n",
    "    data[\"月份\"] = data['日期'].dt.month\n",
    "    data[\"日\"] = data['日期'].dt.day\n",
    "    data[\"星期\"] = data['日期'].dt.weekday\n",
    "    data.drop(columns=['日期'], inplace=True)\n",
    "    data['均价']= (data['最低价格'] + data['最高价格']) / 2\n",
    "    return data\n",
    "\n",
    "def data_tezheng(data):\n",
    "    \"\"\"\n",
    "    数据特征分析和特征处理以及选择\n",
    "    data: 数据集\n",
    "    return: 特征处理后的,仅仅包含所需要的特征x和目标y的数据集\n",
    "    *注意在时间维度上，去除日期会导致相似/相同 记录的重复，可能会造成数据泄露---需要对y进行处理，去除重复的y（删除/均值）\n",
    "    \"\"\"\n",
    "    data = data_Hot_Deck_Imputation(data) # 热卡填补法\n",
    "    # 重新包装中N与E的数量比过大，E只有5条记录，且存在至少两个特征的缺失值，故不考虑重新包装特征\n",
    "    data.drop(columns=['重新包装'], inplace=True)\n",
    "    # 删除全部为空的列\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    # 删除空值数量过多的列---该类数据的信息量太少，对后续分析无意义\n",
    "    data = data.dropna(thresh=len(data)*0.2, axis=1) #thresh参数:指定保留的行或列中至少应包含的非缺失值的数量(可以容忍的缺失值数量)\n",
    "    data['均价']= (data['最低价格'] + data['最高价格']) / 2\n",
    "    # 去除其他无法填充，依旧存在空值的记录\n",
    "    data = data.dropna(axis=0, how='any')\n",
    "    # 去除异常值\n",
    "    q1 = data['均价'].quantile(0.25)\n",
    "    q3 = data['均价'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    data = data[~((data['均价'] < (q1 - 1.5 * iqr)) | (data['均价'] > (q3 + 1.5 * iqr)))]\n",
    "    # data = data[[ '月份', '星期', '物品尺寸', '包装', '品种', '产地', '均价']]\n",
    "    # x=data[[ '月份', '星期', '物品尺寸', '包装', '品种', '产地']]\n",
    "    # y=data['均价']\n",
    "    # print(data.info())\n",
    "    return data\n",
    "\n",
    "def data_Hot_Deck_Imputation(data):\n",
    "    \"\"\"\n",
    "    热卡填补法-手动查找\n",
    "    \"\"\"\n",
    "    print(\"=\"*50,'热卡填补法-手动',\"=\"*50)\n",
    "    print('检查填充前空值情况：',data['产地'].isnull().sum()) \n",
    "    # 使用空值记录中其他存在的特征相同的记录的品种 填充，如 城市名称、包装、产地、（颜色、尺寸）------ 使用其中出现频率最高的品种属性填充\n",
    "    data['产地'].fillna(data.groupby(['城市名称','包装','物品尺寸','品种'])['产地'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan), inplace=True)\n",
    "    print('检查填充后是否还有空值：',data['产地'].isnull().sum())\n",
    "    print('检查填充前空值情况：',data['品种'].isnull().sum()) \n",
    "    # 使用空值记录中其他存在的特征相同的记录的品种 填充，如 城市名称、包装、产地、（颜色、尺寸）------ 使用其中出现频率最高的品种属性填充\n",
    "    data['品种'].fillna(data.groupby(['城市名称','包装','物品尺寸','产地','最低价格'])['品种'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan), inplace=True)\n",
    "    print('检查填充后是否还有空值：',data['品种'].isnull().sum()) # 检查填充后是否还有空值\n",
    "    data['品种'].fillna(data.groupby(['城市名称','包装','物品尺寸','产地'])['品种'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan), inplace=True)\n",
    "    print('放宽填要求后是否还有空值：',data['品种'].isnull().sum()) # 品牌成功填充\n",
    "    return data\n",
    "\n",
    "def data_str_bianma():\n",
    "    \"\"\"\n",
    "    对于非数值类标签/离散标签进行 编码 ： 独热编码，\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "data = pd.read_csv('../data/US-pumpkins.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=date_chuli0(data_rename(data))\n",
    "# 数据特征分析和特征处理以及选择\n",
    "data = data_tezheng(data)\n",
    "data\n",
    "# 保存特征处理后的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_list = [column for column in data.columns if data[column].dtype == 'object']\n",
    "column_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ca0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from category_encoders import TargetEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "def encode_features(x, encoding_method='one_hot'):\n",
    "    \"\"\"对离散特征进行编码\"\"\"\n",
    "    categorical_features = ['物品尺寸', '包装', '品种', '产地']\n",
    "    \n",
    "    if encoding_method == 'one_hot':\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "    elif encoding_method == 'ordinal':\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('cat', OrdinalEncoder(), categorical_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"不支持的编码方法\")\n",
    "    return preprocessor\n",
    "\n",
    "def get_model(model_name, random_state=42):\n",
    "    \"\"\"根据模型名称获取模型实例\"\"\"\n",
    "    models = {\n",
    "        'decision_tree': DecisionTreeRegressor(random_state=random_state), # 回归树\n",
    "        'random_forest': RandomForestRegressor(random_state=random_state), # 随机森林\n",
    "        'linear_regression': LinearRegression(), # 线性回归\n",
    "        'lgbm': lgb.LGBMRegressor(random_state=random_state), # LightGBM回归\n",
    "        'xgboost': xgb.XGBRegressor(random_state=random_state) # XGBoost回归\n",
    "    }\n",
    "    \n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"不支持的模型: {model_name}\")\n",
    "    return models[model_name]\n",
    "\n",
    "def get_param_grid(model_name):\n",
    "    \"\"\"获取模型对应的超参数网格\"\"\"\n",
    "    param_grids = {\n",
    "        'decision_tree': {\n",
    "            'regressor__max_depth': [5, 10, 20, None],\n",
    "            'regressor__min_samples_split': [2, 5, 10],\n",
    "            'regressor__min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__max_depth': [5, 10, 20, None],\n",
    "            'regressor__max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "        'linear_regression': {\n",
    "            'regressor__fit_intercept': [True, False]\n",
    "        },\n",
    "        'ridge': {\n",
    "            'regressor__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "            'regressor__solver': ['auto', 'svd', 'cholesky']\n",
    "        },\n",
    "        'lasso': {\n",
    "            'regressor__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "            'regressor__selection': ['cyclic', 'random']\n",
    "        },\n",
    "        'lgbm': {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'regressor__num_leaves': [31, 63, 127],\n",
    "            'regressor__max_depth': [5, 10, 20, -1],\n",
    "            'regressor__subsample': [0.8, 1.0]\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'regressor__max_depth': [3, 6, 9],\n",
    "            'regressor__subsample': [0.8, 1.0],\n",
    "            'regressor__colsample_bytree': [0.8, 1.0],\n",
    "            'regressor__gamma': [0, 0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    return param_grids.get(model_name, {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_hyperparameters(x, y, preprocessor, model_name, n_splits=5):\n",
    "    \"\"\"使用网格搜索寻找最佳超参数\"\"\"\n",
    "    model = get_model(model_name)\n",
    "    \n",
    "    # 创建评估管道\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # 获取超参数网格\n",
    "    param_grid = get_param_grid(model_name)\n",
    "    \n",
    "    # 对线性模型添加标准化\n",
    "    if model_name in ['linear_regression', 'ridge', 'lasso']:\n",
    "        pipeline.steps.insert(1, ('scaler', StandardScaler(with_mean=False)))\n",
    "    \n",
    "    # 设置网格搜索\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=n_splits, scoring='neg_mean_squared_error', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 执行网格搜索\n",
    "    grid_search.fit(x, y)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "def kfold_cross_validate(model, x, y, n_splits=5, random_state=42):\n",
    "    \"\"\"执行K折交叉验证并返回详细结果\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(x)):\n",
    "        x_train, x_test = x.iloc[train_idx], x.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_size': len(train_idx),\n",
    "            'test_size': len(test_idx),\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'model_params': model.get_params()['regressor'].__dict__\n",
    "        })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40466f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_validation_results(results, model_name, encoding_method, best_params=None):\n",
    "    \"\"\"打印详细的验证结果\"\"\"\n",
    "    print(f\"\\n=== {model_name.upper()} + {encoding_method.upper()} 验证结果 ===\")\n",
    "    \n",
    "    if best_params:\n",
    "        print(\"\\n最佳超参数:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  - {param.replace('regressor__', '')}: {value}\")\n",
    "    \n",
    "    print(\"\\n每折验证结果:\")\n",
    "    for fold_result in results:\n",
    "        print(f\"\\n第 {fold_result['fold']} 折:\")\n",
    "        print(f\"  训练集大小: {fold_result['train_size']}\")\n",
    "        print(f\"  测试集大小: {fold_result['test_size']}\")\n",
    "        print(f\"  MSE: {fold_result['mse']:.4f}\")\n",
    "        print(f\"  RMSE: {fold_result['rmse']:.4f}\")\n",
    "        print(f\"  MAE: {fold_result['mae']:.4f}\")\n",
    "        print(f\"  R²: {fold_result['r2']:.4f}\")\n",
    "        \n",
    "        print(\"\\n  当前模型参数:\")\n",
    "        for param, value in fold_result['model_params'].items():\n",
    "            if not param.endswith('_') and not callable(value):\n",
    "                print(f\"    - {param}: {value}\")\n",
    "    \n",
    "    all_mse = [r['mse'] for r in results]\n",
    "    all_rmse = [r['rmse'] for r in results]\n",
    "    all_mae = [r['mae'] for r in results]\n",
    "    all_r2 = [r['r2'] for r in results]\n",
    "    \n",
    "    print(\"\\n总体性能:\")\n",
    "    print(f\"  平均MSE: {np.mean(all_mse):.4f} ± {np.std(all_mse):.4f}\")\n",
    "    print(f\"  平均RMSE: {np.mean(all_rmse):.4f} ± {np.std(all_rmse):.4f}\")\n",
    "    print(f\"  平均MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")\n",
    "    print(f\"  平均R²: {np.mean(all_r2):.4f} ± {np.std(all_r2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af739ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc395140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b04a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c018f87d",
   "metadata": {},
   "source": [
    "# 管道使用和k折交叉验证的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets  import load_iris \n",
    "from sklearn.preprocessing  import StandardScaler, LabelEncoder \n",
    "from sklearn.model_selection  import KFold, cross_val_score \n",
    "from sklearn.pipeline  import Pipeline \n",
    "from sklearn.linear_model  import LogisticRegression \n",
    "from sklearn.compose  import ColumnTransformer \n",
    " \n",
    "# 加载数据集 \n",
    "data = load_iris()\n",
    "X, y = data.data,  data.target  \n",
    "\n",
    "# 创建预处理和模型的管道 \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), [0, 1, 2, 3])  # 对数值特征归一化 \n",
    "    ]\n",
    ")\n",
    " \n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "# 5折交叉验证 \n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold, scoring='accuracy')\n",
    " \n",
    "print(f\"交叉验证平均准确率: {scores.mean():.2f}  ± {scores.std():.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3be699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 创建模拟数据集\n",
    "# 生成1000个样本，包含数值特征、名义分类特征和有序分类特征\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# 数值特征\n",
    "age = np.random.normal(40, 10, n_samples)\n",
    "income = np.random.normal(50000, 15000, n_samples)\n",
    "\n",
    "# 名义分类特征（无序）\n",
    "education = np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], size=n_samples)\n",
    "city = np.random.choice(['New York', 'London', 'Tokyo', 'Paris'], size=n_samples)\n",
    "\n",
    "# 有序分类特征（有内在顺序）\n",
    "experience_level = np.random.choice(['Junior', 'Mid-Level', 'Senior', 'Executive'], \n",
    "                                   size=n_samples, p=[0.3, 0.4, 0.2, 0.1])\n",
    "satisfaction = np.random.choice(['Very Low', 'Low', 'Medium', 'High', 'Very High'], \n",
    "                               size=n_samples)\n",
    "\n",
    "# 目标变量\n",
    "target = (age * 0.1 + (income > 55000).astype(int) + \n",
    "          np.where(education == 'PhD', 1, 0) + \n",
    "          np.where(experience_level == 'Executive', 2, 0) > 2).astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'income': income,\n",
    "    'education': education,\n",
    "    'city': city,\n",
    "    'experience_level': experience_level,\n",
    "    'satisfaction': satisfaction,\n",
    "    'target': target\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 定义特征类型\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# 指定特征类型\n",
    "numerical_features = ['age', 'income']  # 数值特征\n",
    "nominal_features = ['education', 'city']  # 名义分类特征（无序）\n",
    "ordinal_features = ['experience_level', 'satisfaction']  # 有序分类特征\n",
    "\n",
    "# 3. 定义有序特征的类别顺序\n",
    "# 这很重要，因为OrdinalEncoder需要知道类别的内在顺序\n",
    "experience_categories = ['Junior', 'Mid-Level', 'Senior', 'Executive']\n",
    "satisfaction_categories = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "# 4. 创建列转换器\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # 数值特征：标准化\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        \n",
    "        # 名义分类特征：One-Hot编码\n",
    "        ('nom', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), nominal_features),\n",
    "        \n",
    "        # 有序分类特征：Ordinal编码\n",
    "        ('ord', OrdinalEncoder(\n",
    "            categories=[experience_categories, satisfaction_categories]),\n",
    "         ordinal_features)\n",
    "    ],\n",
    "    remainder='drop'  # 忽略其他列（本例中没有其他列）\n",
    ")\n",
    "\n",
    "# 5. 创建完整管道\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # 特征预处理\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # 分类器\n",
    "])\n",
    "\n",
    "# 6. 使用5折交叉验证评估模型\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"交叉验证准确率:\", scores)\n",
    "print(f\"平均准确率: {np.mean(scores):.4f} (±{np.std(scores):.4f})\")\n",
    "\n",
    "# 7. 查看预处理后的特征名称（可选）\n",
    "# 拟合一次以获取特征名称\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# 获取One-Hot编码后的特征名称\n",
    "onehot_columns = pipeline.named_steps['preprocessor'].named_transformers_['nom'].get_feature_names_out(nominal_features)\n",
    "\n",
    "# 获取所有特征名称\n",
    "all_features = (\n",
    "    list(numerical_features) + \n",
    "    list(onehot_columns) + \n",
    "    ordinal_features\n",
    ")\n",
    "\n",
    "print(\"\\n预处理后的特征名称:\")\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1417014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "完整的编码映射字典:\n",
      "==================================================\n",
      "名义特征 (One-Hot编码):\n",
      "  education:\n",
      "    Bachelor → 0\n",
      "    High School → 1\n",
      "    Master → 2\n",
      "    PhD → 3\n",
      "  city:\n",
      "    London → 0\n",
      "    New York → 1\n",
      "    Paris → 2\n",
      "    Tokyo → 3\n",
      "\n",
      "有序特征 (Ordinal编码):\n",
      "  experience_level:\n",
      "    Junior → 0\n",
      "    Mid-Level → 1\n",
      "    Senior → 2\n",
      "    Executive → 3\n",
      "  satisfaction:\n",
      "    Very Low → 0\n",
      "    Low → 1\n",
      "    Medium → 2\n",
      "    High → 3\n",
      "    Very High → 4\n",
      "\n",
      "数值特征 (标准化参数):\n",
      "  age: 均值=40.25, 标准差=9.80\n",
      "  income: 均值=51281.86, 标准差=14931.28\n",
      "\n",
      "==================================================\n",
      "交叉验证准确率: 0.9900 (±0.0064)\n",
      "==================================================\n",
      "测试集准确率: 1.0000\n",
      "\n",
      "管道和编码映射已保存为 'model_pipeline.pkl' 和 'encoding_mappings.pkl'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# 1. 创建模拟数据集\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# 数值特征\n",
    "age = np.random.normal(40, 10, n_samples)\n",
    "income = np.random.normal(50000, 15000, n_samples)\n",
    "\n",
    "# 名义分类特征\n",
    "education = np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], size=n_samples)\n",
    "city = np.random.choice(['New York', 'London', 'Tokyo', 'Paris'], size=n_samples)\n",
    "\n",
    "# 有序分类特征\n",
    "experience_level = np.random.choice(['Junior', 'Mid-Level', 'Senior', 'Executive'], \n",
    "                                   size=n_samples, p=[0.3, 0.4, 0.2, 0.1])\n",
    "satisfaction = np.random.choice(['Very Low', 'Low', 'Medium', 'High', 'Very High'], \n",
    "                               size=n_samples)\n",
    "\n",
    "# 目标变量\n",
    "target = (age * 0.1 + (income > 55000).astype(int) + \n",
    "          np.where(education == 'PhD', 1, 0) + \n",
    "          np.where(experience_level == 'Executive', 2, 0) > 2).astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'income': income,\n",
    "    'education': education,\n",
    "    'city': city,\n",
    "    'experience_level': experience_level,\n",
    "    'satisfaction': satisfaction,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "# 2. 定义特征类型\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 指定特征类型\n",
    "numerical_features = ['age', 'income']\n",
    "nominal_features = ['education', 'city']  # 名义分类特征\n",
    "ordinal_features = ['experience_level', 'satisfaction']  # 有序分类特征\n",
    "\n",
    "# 3. 定义有序特征的类别顺序\n",
    "\n",
    "experience_categories = ['Junior', 'Mid-Level', 'Senior', 'Executive']\n",
    "satisfaction_categories = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "ordinal_features_zd={'experience_level':experience_categories, 'satisfaction':satisfaction_categories}\n",
    "\n",
    "# 4. 创建编码映射字典（用于保存所有映射关系）\n",
    "encoding_mappings = {\n",
    "    'nominal': {},\n",
    "    'ordinal': {},\n",
    "    'numerical': {}\n",
    "}\n",
    "\n",
    "# # 5. 创建列转换器\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         # 数值特征：标准化\n",
    "#         ('num', StandardScaler(), numerical_features),\n",
    "        \n",
    "#         # 名义分类特征：One-Hot编码\n",
    "#         ('nom', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), nominal_features),\n",
    "        \n",
    "#         # 有序分类特征：Ordinal编码\n",
    "#         ('ord', OrdinalEncoder(\n",
    "#             categories=[ordinal_features_zd['experience_level'], ordinal_features_zd['satisfaction']]), \n",
    "#          ordinal_features)\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "def pipeline_preprocessor_ColumnTransformer(numerical_features, nominal_features, ordinal_features,ordinal_features_zd, data):\n",
    "    # ordinal_features_zd={'experience_level':experience_categories, 'satisfaction':satisfaction_categories}\n",
    "\n",
    "    # 4. 创建编码映射字典（用于保存所有映射关系）\n",
    "    encoding_mappings = {\n",
    "        'nominal': {},\n",
    "        'ordinal': {},\n",
    "        'numerical': {}\n",
    "    }\n",
    "\n",
    "    # 5. 创建列转换器\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # 数值特征：标准化\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            \n",
    "            # 名义分类特征：One-Hot编码\n",
    "            ('nom', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), nominal_features),\n",
    "            \n",
    "            # 有序分类特征：Ordinal编码\n",
    "            ('ord', OrdinalEncoder(\n",
    "                categories=[ordinal_features_zd[feature] for feature in ordinal_features]), \n",
    "            ordinal_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    return preprocessor\n",
    "preprocessor=pipeline_preprocessor_ColumnTransformer(numerical_features, nominal_features, ordinal_features,ordinal_features_zd, data)\n",
    "\n",
    "# 6. 创建完整管道\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 7. 训练管道并获取编码映射\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 8. 提取并保存映射关系\n",
    "# 获取One-Hot编码器的映射\n",
    "onehot_encoder = pipeline.named_steps['preprocessor'].named_transformers_['nom'] \n",
    "for i, feature in enumerate(nominal_features):\n",
    "    categories = onehot_encoder.categories_[i]\n",
    "    mapping = {category: idx for idx, category in enumerate(categories)}\n",
    "    encoding_mappings['nominal'][feature] = mapping\n",
    "\n",
    "# 获取Ordinal编码器的映射\n",
    "ordinal_encoder = pipeline.named_steps['preprocessor'].named_transformers_['ord']\n",
    "for i, feature in enumerate(ordinal_features):\n",
    "    categories = ordinal_encoder.categories_[i]\n",
    "    mapping = {category: idx for idx, category in enumerate(categories)}\n",
    "    encoding_mappings['ordinal'][feature] = mapping\n",
    "\n",
    "# 获取数值特征的标准化参数\n",
    "scaler = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    mapping = {\n",
    "        'mean': scaler.mean_[i],\n",
    "        'std': scaler.scale_[i]\n",
    "    }\n",
    "    encoding_mappings['numerical'][feature] = mapping\n",
    "\n",
    "# 9. 输出映射字典\n",
    "print(\"=\"*50)\n",
    "print(\"完整的编码映射字典:\")\n",
    "print(\"=\"*50)\n",
    "print(\"名义特征 (One-Hot编码):\")\n",
    "for feature, mapping in encoding_mappings['nominal'].items():\n",
    "    print(f\"  {feature}:\")\n",
    "    for category, code in mapping.items():\n",
    "        print(f\"    {category} → {code}\")\n",
    "\n",
    "print(\"\\n有序特征 (Ordinal编码):\")\n",
    "for feature, mapping in encoding_mappings['ordinal'].items():\n",
    "    print(f\"  {feature}:\")\n",
    "    for category, code in mapping.items():\n",
    "        print(f\"    {category} → {code}\")\n",
    "\n",
    "print(\"\\n数值特征 (标准化参数):\")\n",
    "for feature, params in encoding_mappings['numerical'].items():\n",
    "    print(f\"  {feature}: 均值={params['mean']:.2f}, 标准差={params['std']:.2f}\")\n",
    "\n",
    "# 10. 使用5折交叉验证评估模型\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"交叉验证准确率: {np.mean(scores):.4f} (±{np.std(scores):.4f})\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 11. 在测试集上评估最终模型\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "print(f\"测试集准确率: {test_score:.4f}\")\n",
    "\n",
    "# 12. 保存管道和映射字典\n",
    "joblib.dump(pipeline, 'model_pipeline.pkl')\n",
    "joblib.dump(encoding_mappings, 'encoding_mappings.pkl')\n",
    "\n",
    "print(\"\\n管道和编码映射已保存为 'model_pipeline.pkl' 和 'encoding_mappings.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48737cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用前面保存的管道和映射字典进行预测\n",
    "pipeline = joblib.load('model_pipeline.pkl')\n",
    "encoding_mappings = joblib.load('encoding_mappings.pkl')\n",
    "\n",
    "# 13. 使用保存的映射字典进行预测\n",
    "# 示例输入数据\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [35],\n",
    "    'income': [60000],\n",
    "    'education': ['PhD'],\n",
    "    'city': ['New York'],\n",
    "    'experience_level': ['Executive'],\n",
    "    'satisfaction': ['Very High']\n",
    "})\n",
    "\n",
    "\n",
    "# 对新数据进行编码---错误，因为管道pipeline已经完成编码，无需上述在进行手动编码\n",
    "# encoded_data = new_data.copy()\n",
    "# for feature, mapping in encoding_mappings['nominal'].items():\n",
    "#     encoded_data[feature] = encoded_data[feature].map(mapping)\n",
    "\n",
    "# for feature, mapping in encoding_mappings['ordinal'].items():\n",
    "#     encoded_data[feature] = encoded_data[feature].map(mapping)\n",
    "# encoded_data[numerical_features] = encoded_data[numerical_features].apply(lambda x: (x - encoding_mappings['numerical'][x.name]['mean']) / encoding_mappings['numerical'][x.name]['std'])\n",
    "\n",
    "# # 使用保存的管道进行预测---错误，因为管道pipeline已经完成编码，无需上述在进行手动编码\n",
    "# prediction = pipeline.predict(encoded_data)\n",
    "# print(f\"预测结果: {prediction[0]}\")\n",
    "# 只需标准化数值特征（如果 pipeline 里没做），否则直接用原始 new_data\n",
    "prediction = pipeline.predict(new_data)\n",
    "print(f\"预测结果: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879828ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in d:\\anaconda_24\\envs\\all_can_cpu\\lib\\site-packages (2.1.1)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda_24\\envs\\all_can_cpu\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda_24\\envs\\all_can_cpu\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/124.9 MB 3.4 MB/s eta 0:00:38\n",
      "    --------------------------------------- 2.4/124.9 MB 6.7 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 5.0/124.9 MB 9.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 6.3/124.9 MB 8.4 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 8.9/124.9 MB 9.4 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 11.3/124.9 MB 9.8 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 13.6/124.9 MB 10.0 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 16.3/124.9 MB 10.3 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 18.6/124.9 MB 10.5 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 20.4/124.9 MB 10.3 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 22.8/124.9 MB 10.3 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 25.2/124.9 MB 10.4 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 27.8/124.9 MB 10.6 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 29.1/124.9 MB 10.4 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 30.9/124.9 MB 10.2 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 32.5/124.9 MB 10.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 33.6/124.9 MB 10.0 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 34.9/124.9 MB 9.5 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 37.5/124.9 MB 9.6 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 39.1/124.9 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 40.6/124.9 MB 9.5 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 42.2/124.9 MB 9.4 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 44.0/124.9 MB 9.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 45.6/124.9 MB 9.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 47.2/124.9 MB 9.2 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 48.8/124.9 MB 9.2 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 50.3/124.9 MB 9.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 52.2/124.9 MB 9.1 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 53.7/124.9 MB 9.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 55.3/124.9 MB 9.0 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 57.1/124.9 MB 8.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 58.7/124.9 MB 8.9 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 60.3/124.9 MB 8.9 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 61.9/124.9 MB 8.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 63.4/124.9 MB 8.8 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 65.0/124.9 MB 8.8 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 66.8/124.9 MB 8.8 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 68.4/124.9 MB 8.7 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 70.0/124.9 MB 8.7 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 71.6/124.9 MB 8.7 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 73.1/124.9 MB 8.7 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 74.7/124.9 MB 8.6 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 76.5/124.9 MB 8.6 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 78.1/124.9 MB 8.6 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 80.0/124.9 MB 8.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 81.5/124.9 MB 8.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 83.1/124.9 MB 8.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 83.4/124.9 MB 8.6 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 85.5/124.9 MB 8.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 87.8/124.9 MB 8.5 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 89.7/124.9 MB 8.5 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 91.2/124.9 MB 8.5 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 92.8/124.9 MB 8.5 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 94.6/124.9 MB 8.5 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 96.2/124.9 MB 8.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 97.8/124.9 MB 8.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 99.4/124.9 MB 8.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 100.9/124.9 MB 8.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 101.7/124.9 MB 8.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 101.7/124.9 MB 8.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 103.8/124.9 MB 8.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 106.2/124.9 MB 8.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 108.5/124.9 MB 8.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 110.6/124.9 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 112.2/124.9 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 114.0/124.9 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 115.6/124.9 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 117.2/124.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 118.5/124.9 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 119.5/124.9 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  121.9/124.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  123.7/124.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  124.8/124.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 124.9/124.9 MB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 2.1.1\n",
      "    Uninstalling xgboost-2.1.1:\n",
      "      Successfully uninstalled xgboost-2.1.1\n",
      "Successfully installed xgboost-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cb561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in d:\\anaconda_24\\envs\\all_can_cpu\\lib\\site-packages (0.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce01e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function graphviz.backend.upstream_version.version() -> Tuple[int, ...]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_can_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
